# RTX 3070 Ti ë²¤ì¹˜ë§ˆí¬ ê¸°ë°˜ ìµœì í™” ì„¤ì •

import torch
from dataclasses import dataclass
from multiprocessing import cpu_count

@dataclass
class RTX3070OptimizedConfig:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ê¸°ë°˜ RTX 3070 Ti ìµœì í™” ì„¤ì •"""
    # GPU ê¸°ë³¸ ì„¤ì •
    embedding_model: str = "dragonkue/bge-m3-ko"
    use_gpu: bool = True
    device: str = "cuda"
    
    # ë²¤ì¹˜ë§ˆí¬ ê¸°ë°˜ ìµœì í™”
    batch_size: int = 128  # ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì˜ ê· í˜•ì 
    max_batch_size: int = 256  # ìµœëŒ€ ì„±ëŠ¥ ë°°ì¹˜ í¬ê¸°
    fallback_batch_size: int = 64  # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ëŒ€ì•ˆ
    
    max_sequence_length: int = 512
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì • (40 SM í™œìš©)
    max_workers: int = min(8, cpu_count())  # SM ê°œìˆ˜ë¥¼ ê³ ë ¤í•œ ì›Œì»¤ ìˆ˜ ì¦ê°€
    chunk_size: int = 1500  # ì²˜ë¦¬ëŸ‰ ì¦ê°€ì— ë§ì¶° ì²­í¬ í¬ê¸° ì¦ê°€
    
    # ë©”ëª¨ë¦¬ ê´€ë¦¬ (8.6GB VRAM í™œìš©)
    save_frequency: int = 750  # ì²˜ë¦¬ëŸ‰ ì¦ê°€ì— ë§ì¶° ì¡°ì •
    enable_mixed_precision: bool = True
    memory_fraction: float = 0.85  # 8.6GBì˜ 85% ì‚¬ìš© (ë” ë³´ìˆ˜ì )
    
    # ì„±ëŠ¥ ìµœì í™”
    pin_memory: bool = True
    num_workers_dataloader: int = 6  # ëŠ˜ë¦° ì›Œì»¤ ìˆ˜ì— ë§ì¶° ì¡°ì •
    
    # ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
    adaptive_batch_size: bool = True
    performance_target: float = 2000000  # ëª©í‘œ ì²˜ë¦¬ëŸ‰ (samples/sec)

def dynamic_batch_optimization():
    """ë™ì  ë°°ì¹˜ í¬ê¸° ìµœì í™”"""
    if not torch.cuda.is_available():
        return 32
    
    # ì‹¤ì œ ì„ë² ë”© ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
    test_sizes = [64, 128, 192, 256, 320]
    best_batch_size = 64
    best_throughput = 0
    
    print("ğŸ” ì‹¤ì œ ì„ë² ë”© ëª¨ë¸ë¡œ ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ëŠ” ì¤‘...")
    
    try:
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer("dragonkue/bge-m3-ko")
        model = model.to("cuda")
        model.half()  # FP16ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
        
        # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ì¤€ë¹„
        test_texts = ["í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤." * 10] * 320  # ìµœëŒ€ ë°°ì¹˜ í¬ê¸°ë§Œí¼
        
        import time
        
        for batch_size in test_sizes:
            try:
                # 3ë²ˆ ì¸¡ì • í›„ í‰ê· 
                times = []
                for _ in range(3):
                    batch_texts = test_texts[:batch_size]
                    
                    torch.cuda.synchronize()
                    start_time = time.time()
                    
                    with torch.no_grad():
                        embeddings = model.encode(
                            batch_texts,
                            batch_size=batch_size,
                            device="cuda",
                            show_progress_bar=False,
                            convert_to_numpy=True
                        )
                    
                    torch.cuda.synchronize()
                    end_time = time.time()
                    
                    times.append(end_time - start_time)
                    torch.cuda.empty_cache()
                
                avg_time = sum(times) / len(times)
                throughput = batch_size / avg_time
                
                print(f"   ë°°ì¹˜ í¬ê¸° {batch_size:3d}: {throughput:8.1f} embeddings/sec")
                
                if throughput > best_throughput:
                    best_throughput = throughput
                    best_batch_size = batch_size
                
            except torch.cuda.OutOfMemoryError:
                print(f"   ë°°ì¹˜ í¬ê¸° {batch_size:3d}: ë©”ëª¨ë¦¬ ë¶€ì¡±")
                break
            except Exception as e:
                print(f"   ë°°ì¹˜ í¬ê¸° {batch_size:3d}: ì˜¤ë¥˜ - {e}")
                continue
        
        print(f"âœ… ìµœì  ë°°ì¹˜ í¬ê¸°: {best_batch_size} (ì²˜ë¦¬ëŸ‰: {best_throughput:.1f} embeddings/sec)")
        return best_batch_size
        
    except Exception as e:
        print(f"âŒ ë™ì  ìµœì í™” ì‹¤íŒ¨: {e}")
        return 128  # ê¸°ë³¸ê°’ ë°˜í™˜

class AdaptiveRTX3070Chunker:
    """ì ì‘í˜• RTX 3070 Ti ì²­í‚¹ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.config = RTX3070OptimizedConfig()
        self.model = None
        self.current_batch_size = self.config.batch_size
        
        # GPU ë©”ëª¨ë¦¬ ìµœì í™”
        self._optimize_gpu_memory()
        
        # ë™ì  ë°°ì¹˜ í¬ê¸° ìµœì í™”
        if self.config.adaptive_batch_size:
            optimal_batch_size = dynamic_batch_optimization()
            self.current_batch_size = optimal_batch_size
            self.config.batch_size = optimal_batch_size
        
        print(f"ğŸš€ ì ì‘í˜• RTX 3070 Ti ì²­í‚¹ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        print(f"ğŸ“Š ìµœì í™”ëœ ì„¤ì •:")
        print(f"   ìµœì  ë°°ì¹˜ í¬ê¸°: {self.current_batch_size}")
        print(f"   ì›Œì»¤ ìˆ˜: {self.config.max_workers}")
        print(f"   ì²­í¬ í¬ê¸°: {self.config.chunk_size}")
        print(f"   ë©”ëª¨ë¦¬ í• ë‹¹: {self.config.memory_fraction*100:.1f}%")
    
    def _optimize_gpu_memory(self):
        """GPU ë©”ëª¨ë¦¬ ìµœì í™”"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.set_per_process_memory_fraction(self.config.memory_fraction)
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.enabled = True
            
            gpu_props = torch.cuda.get_device_properties(0)
            total_vram = gpu_props.total_memory / 1e9
            allocated_vram = total_vram * self.config.memory_fraction
            
            print(f"ğŸ¯ GPU ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ!")
            print(f"   ì´ VRAM: {total_vram:.1f}GB")
            print(f"   í• ë‹¹ VRAM: {allocated_vram:.1f}GB")
    
    def process_with_adaptive_batching(self, chunks, output_dir="output"):
        """ì ì‘í˜• ë°°ì¹˜ ì²˜ë¦¬"""
        self.initialize_model()
        
        total_chunks = len(chunks)
        embedded_chunks = []
        current_batch_size = self.current_batch_size
        
        print(f"ğŸš€ ì ì‘í˜• ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {total_chunks}ê°œ ì²­í¬")
        print(f"   ì´ˆê¸° ë°°ì¹˜ í¬ê¸°: {current_batch_size}")
        
        from tqdm import tqdm
        import time
        
        processing_times = []
        
        for i in tqdm(range(0, total_chunks, current_batch_size), desc="ì„ë² ë”© ì§„í–‰"):
            batch_chunks = chunks[i:i+current_batch_size]
            
            try:
                start_time = time.time()
                
                # ë°°ì¹˜ ì²˜ë¦¬
                texts = [chunk['text'][:self.config.max_sequence_length] for chunk in batch_chunks]
                
                with torch.no_grad():
                    with torch.cuda.amp.autocast(enabled=self.config.enable_mixed_precision):
                        embeddings = self.model.encode(
                            texts,
                            batch_size=current_batch_size,
                            device=self.config.device,
                            show_progress_bar=False,
                            convert_to_numpy=True,
                            normalize_embeddings=True
                        )
                
                # ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
                processing_time = time.time() - start_time
                processing_times.append(processing_time)
                
                # ì„ë² ë”© ì¶”ê°€
                for j, chunk in enumerate(batch_chunks):
                    chunk['embedding'] = embeddings[j].tolist()
                    embedded_chunks.append(chunk)
                
                # ì„±ëŠ¥ ê¸°ë°˜ ë°°ì¹˜ í¬ê¸° ì¡°ì •
                if len(processing_times) >= 5:  # 5ë²ˆë§ˆë‹¤ ì„±ëŠ¥ ì²´í¬
                    avg_time = sum(processing_times[-5:]) / 5
                    current_throughput = current_batch_size / avg_time
                    
                    # ì²˜ë¦¬ëŸ‰ì´ ëª©í‘œë³´ë‹¤ ë‚®ìœ¼ë©´ ë°°ì¹˜ í¬ê¸° ì¦ê°€ ì‹œë„
                    if current_throughput < self.config.performance_target and current_batch_size < self.config.max_batch_size:
                        current_batch_size = min(current_batch_size + 32, self.config.max_batch_size)
                        print(f"ğŸ“ˆ ë°°ì¹˜ í¬ê¸° ì¦ê°€: {current_batch_size}")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if (i + current_batch_size) % (current_batch_size * 3) == 0:
                    torch.cuda.empty_cache()
                
                # ì¤‘ê°„ ì €ì¥
                if len(embedded_chunks) % self.config.save_frequency == 0:
                    self._save_intermediate(embedded_chunks, output_dir, len(embedded_chunks))
                
            except torch.cuda.OutOfMemoryError:
                print(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±! ë°°ì¹˜ í¬ê¸° {current_batch_size} â†’ {self.config.fallback_batch_size}")
                current_batch_size = self.config.fallback_batch_size
                torch.cuda.empty_cache()
                continue
                
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                for chunk in batch_chunks:
                    chunk['embedding'] = None
                    embedded_chunks.append(chunk)
        
        # ì„±ëŠ¥ í†µê³„ ì¶œë ¥
        if processing_times:
            avg_processing_time = sum(processing_times) / len(processing_times)
            avg_throughput = self.current_batch_size / avg_processing_time
            
            print(f"\nğŸ“Š ì²˜ë¦¬ ì„±ëŠ¥ í†µê³„:")
            print(f"   í‰ê·  ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„: {avg_processing_time:.3f}ì´ˆ")
            print(f"   í‰ê·  ì²˜ë¦¬ëŸ‰: {avg_throughput:.1f} embeddings/sec")
            print(f"   ìµœì¢… ë°°ì¹˜ í¬ê¸°: {current_batch_size}")
        
        torch.cuda.empty_cache()
        return embedded_chunks
    
    def initialize_model(self):
        """ìµœì í™”ëœ ëª¨ë¸ ì´ˆê¸°í™”"""
        if self.model is None:
            print("ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘... (ì ì‘í˜• ìµœì í™”)")
            
            from sentence_transformers import SentenceTransformer
            self.model = SentenceTransformer(self.config.embedding_model)
            self.model = self.model.to(self.config.device)
            
            if self.config.enable_mixed_precision:
                self.model.half()
                print("âœ… FP16 í˜¼í•© ì •ë°€ë„ í™œì„±í™”")
            
            print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    def _save_intermediate(self, chunks, output_dir, count):
        """ì¤‘ê°„ ì €ì¥"""
        import os
        import pickle
        
        os.makedirs(output_dir, exist_ok=True)
        filename = os.path.join(output_dir, f"adaptive_rtx3070_{count}.pkl")
        
        with open(filename, 'wb') as f:
            pickle.dump(chunks, f)
        
        print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥: {count}ê°œ ì²­í¬")

# ì‚¬ìš© ì˜ˆì‹œ
def run_adaptive_rtx3070():
    """ì ì‘í˜• RTX 3070 Ti ìµœì í™” ì‹¤í–‰"""
    
    print("ğŸš€ ì ì‘í˜• RTX 3070 Ti ìµœì í™” ì‹œìŠ¤í…œ")
    print("   - ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
    print("   - ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •") 
    print("   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”")
    print("   - ë²¤ì¹˜ë§ˆí¬ ê¸°ë°˜ ì„¤ì •\n")
    
    chunker = AdaptiveRTX3070Chunker()
    
    print("\nğŸš€ ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
    print("ğŸ“ JSON íŒŒì¼ í´ë” ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”:")
    
    return chunker

if __name__ == "__main__":
    run_adaptive_rtx3070()