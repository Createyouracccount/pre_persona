# RTX 3070 Ti ìµœì í™” ì„¤ì •

import torch
from dataclasses import dataclass
from multiprocessing import cpu_count

@dataclass
class RTX3070Config:
    """RTX 3070 Ti ìµœì í™” GPU ì„¤ì •
    ë‚´ GPUì˜ ìµœì í™”ë¥¼ ì•Œì•„ë³´ê¸° ìœ„í•œ ê³¼ì •
    cmd - nvidia-smi ë¥¼ í†µí•´ GPU ìŠ¤í™ í™•ì¸ í›„ ì§„í–‰
    """
    # GPU ê¸°ë³¸ ì„¤ì •
    embedding_model: str = "dragonkue/bge-m3-ko"
    use_gpu: bool = True
    device: str = "cuda"
    
    # RTX 3070 Ti (8GB VRAM) ìµœì í™”
    batch_size: int = 64  # 8GB VRAMì— ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸°
    max_sequence_length: int = 512  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ ì‹œí€€ìŠ¤ ê¸¸ì´ ì œí•œ
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
    max_workers: int = min(6, cpu_count())  # CPU ì½”ì–´ì— ë§ì¶˜ ì›Œì»¤ ìˆ˜
    chunk_size: int = 1000
    
    # ë©”ëª¨ë¦¬ ê´€ë¦¬
    save_frequency: int = 500  # ë” ìì£¼ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
    enable_mixed_precision: bool = True  # FP16ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
    
    # ì„±ëŠ¥ ìµœì í™”
    pin_memory: bool = True  # CPU-GPU ì „ì†¡ ì†ë„ í–¥ìƒ
    num_workers_dataloader: int = 4  # ë°ì´í„° ë¡œë”© ë³‘ë ¬í™”

def optimize_for_rtx3070():
    """RTX 3070 Tië¥¼ ìœ„í•œ ì¶”ê°€ ìµœì í™”"""
    if torch.cuda.is_available():
        # GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
        torch.cuda.empty_cache()
        
        # ë©”ëª¨ë¦¬ ë¶„í•  ì „ëµ ìµœì í™” (8GB GPUìš©)
        torch.cuda.set_per_process_memory_fraction(0.9)  # VRAMì˜ 90% ì‚¬ìš©
        
        # cuDNN ìµœì í™”
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.enabled = True
        
        print("ğŸ¯ RTX 3070 Ti ìµœì í™” ì™„ë£Œ!")
        print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        print(f"   ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸°: 64")
        print(f"   í˜¼í•© ì •ë°€ë„: í™œì„±í™”")
        
        return True
    return False

# ìˆ˜ì •ëœ GPU ê°€ì† ì²­í‚¹ ì‹œìŠ¤í…œ (RTX 3070 Ti ìµœì í™”)
class RTX3070OptimizedChunker:
    """RTX 3070 Ti ìµœì í™” ì²­í‚¹ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.config = RTX3070Config()
        self.model = None
        
        # RTX 3070 Ti ìµœì í™” ì ìš©
        optimize_for_rtx3070()
        
        print("ğŸš€ RTX 3070 Ti ìµœì í™” ì²­í‚¹ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        print(f"ğŸ“Š ì„¤ì •:")
        print(f"   ë°°ì¹˜ í¬ê¸°: {self.config.batch_size}")
        print(f"   ì›Œì»¤ ìˆ˜: {self.config.max_workers}")
        print(f"   ì €ì¥ ë¹ˆë„: {self.config.save_frequency}")
        print(f"   í˜¼í•© ì •ë°€ë„: {self.config.enable_mixed_precision}")
    
    def initialize_model(self):
        """RTX 3070 Ti ìµœì í™” ëª¨ë¸ ì´ˆê¸°í™”"""
        if self.model is None:
            print("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘... (RTX 3070 Ti ìµœì í™”)")
            
            from sentence_transformers import SentenceTransformer
            self.model = SentenceTransformer(self.config.embedding_model)
            
            # GPUë¡œ ëª¨ë¸ ì´ë™
            self.model = self.model.to(self.config.device)
            
            # í˜¼í•© ì •ë°€ë„ í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
            if self.config.enable_mixed_precision:
                self.model.half()  # FP16ìœ¼ë¡œ ë³€í™˜
                print("âœ… FP16 í˜¼í•© ì •ë°€ë„ í™œì„±í™” (ë©”ëª¨ë¦¬ 50% ì ˆì•½)")
            
            print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ (RTX 3070 Ti ìµœì í™”)")
    
    def process_with_memory_management(self, chunks, output_dir="output"):
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ê°•í™”í•œ ì²˜ë¦¬"""
        self.initialize_model()
        
        total_chunks = len(chunks)
        embedded_chunks = []
        
        print(f"ğŸš€ RTX 3070 Ti ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {total_chunks}ê°œ ì²­í¬")
        
        from tqdm import tqdm
        
        for i in tqdm(range(0, total_chunks, self.config.batch_size), desc="ì„ë² ë”© ì§„í–‰"):
            batch_chunks = chunks[i:i+self.config.batch_size]
            
            try:
                # ë°°ì¹˜ ì²˜ë¦¬
                texts = [chunk['text'][:self.config.max_sequence_length] for chunk in batch_chunks]
                
                with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ë¹„í™œì„±í™”
                    # í˜¼í•© ì •ë°€ë„ ì‚¬ìš©
                    with torch.cuda.amp.autocast(enabled=self.config.enable_mixed_precision):
                        embeddings = self.model.encode(
                            texts,
                            batch_size=self.config.batch_size,
                            device=self.config.device,
                            show_progress_bar=False,
                            convert_to_numpy=True,
                            normalize_embeddings=True  # ì •ê·œí™”ë¡œ ì„±ëŠ¥ í–¥ìƒ
                        )
                
                # ì„ë² ë”© ì¶”ê°€
                for j, chunk in enumerate(batch_chunks):
                    chunk['embedding'] = embeddings[j].tolist()
                    embedded_chunks.append(chunk)
                
                # ë” ìì£¼ ë©”ëª¨ë¦¬ ì •ë¦¬ (RTX 3070 Tiìš©)
                if (i + self.config.batch_size) % (self.config.batch_size * 4) == 0:
                    torch.cuda.empty_cache()
                    import gc
                    gc.collect()
                
                # ì¤‘ê°„ ì €ì¥
                if len(embedded_chunks) % self.config.save_frequency == 0:
                    self._save_intermediate(embedded_chunks, output_dir, len(embedded_chunks))
                
            except torch.cuda.OutOfMemoryError:
                print("âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±! ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì…ë‹ˆë‹¤.")
                self.config.batch_size = max(16, self.config.batch_size // 2)
                torch.cuda.empty_cache()
                continue
                
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” ì„ë² ë”© ì—†ì´ ì¶”ê°€
                for chunk in batch_chunks:
                    chunk['embedding'] = None
                    embedded_chunks.append(chunk)
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
        torch.cuda.empty_cache()
        
        return embedded_chunks
    
    def _save_intermediate(self, chunks, output_dir, count):
        """ì¤‘ê°„ ì €ì¥"""
        import os
        import pickle
        
        os.makedirs(output_dir, exist_ok=True)
        filename = os.path.join(output_dir, f"rtx3070_intermediate_{count}.pkl")
        
        with open(filename, 'wb') as f:
            pickle.dump(chunks, f)
        
        print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥: {count}ê°œ ì²­í¬ (RTX 3070 Ti ìµœì í™”)")

# ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í•¨ìˆ˜
def benchmark_rtx3070():
    """RTX 3070 Ti ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    import time
    import torch
    
    if not torch.cuda.is_available():
        print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("ğŸ” RTX 3070 Ti ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
    
    # GPU ì •ë³´
    gpu_props = torch.cuda.get_device_properties(0)
    print(f"ğŸ“Š GPU ì •ë³´:")
    print(f"   ì´ë¦„: {gpu_props.name}")
    print(f"   VRAM: {gpu_props.total_memory / 1e9:.1f}GB")
    print(f"   SM ê°œìˆ˜: {gpu_props.multi_processor_count}")
    
    # ê°„ë‹¨í•œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    test_sizes = [32, 64, 128, 256]
    
    for batch_size in test_sizes:
        try:
            # í…ŒìŠ¤íŠ¸ í…ì„œ ìƒì„±
            test_tensor = torch.randn(batch_size, 768, device='cuda')
            
            # ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            
            with torch.no_grad():
                for _ in range(100):
                    result = torch.matmul(test_tensor, test_tensor.T)
            
            torch.cuda.synchronize()
            end_time = time.time()
            
            processing_time = end_time - start_time
            throughput = (batch_size * 100) / processing_time
            
            print(f"   ë°°ì¹˜ í¬ê¸° {batch_size:3d}: {throughput:7.1f} samples/sec")
            
        except torch.cuda.OutOfMemoryError:
            print(f"   ë°°ì¹˜ í¬ê¸° {batch_size:3d}: ë©”ëª¨ë¦¬ ë¶€ì¡±")
            break
        finally:
            torch.cuda.empty_cache()

def run_rtx3070_optimized():
    """RTX 3070 Ti ìµœì í™” ì‹¤í–‰"""
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark_rtx3070()
    
    # ìµœì í™”ëœ ì²­í‚¹ ì‹œìŠ¤í…œ ì‹¤í–‰
    chunker = RTX3070OptimizedChunker()
    
    return chunker

if __name__ == "__main__":
    run_rtx3070_optimized()